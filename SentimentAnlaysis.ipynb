{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "721274a4",
   "metadata": {},
   "source": [
    "Import Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f1497538",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ਕੁੱਝ ਵਿਦਵਾਨਾਂ ਲਈ ਤਾਂ ‘ਕਹਾਣੀ ਸਹਿਜ ਸਾਧਾਰਣ ਸੱਚ ਨੂੰ ਸਰਲ ਸਪਾਟ ਰੂਪ ਵਿਚ ਕਹਿੰਦੀ ਹੈ’ ਅਤੇ ‘ਏਸ ਕਲਾ ਨੂੰ... ਕਿਸੇ ਗੁੰਝਲਦਾਰ ਸਾਹਿਤ ਸ਼ਾਸਤਰ ਦੀ ਮੁਹਤਾਜ਼ੀ ਨਹੀਂ ।’ ਪਰ ਸਹਿਜ ਸਾਧਾਰਣ ਸੱਚ ਨੂੰ ਨਾ ਕਹਾਣੀ ਸਰਲ ਸਪਾਟ ਰੂਪ ਵਿਚ ਪੇਸ਼ ਕਰਦੀ ਹੈ ਅਤੇ ਨਾ ਹੀ ਇਸ ਦਾ ਸਾਹਿਤ ਸ਼ਾਸਤਰ ਏਨਾ ਸਰਲ ਰਹੇਗਾ । ਜਿਸ ਤਰ੍ਹਾਂ ਕਵਿਤਾ ਕੇਵਲ ਛੰਦ ਪ੍ਰਬੰਧ ਨਹੀਂ ਹੁੰਦੀ, ਇਸੇ ਤਰ੍ਹਾਂ ਕਹਾਣੀ ਕੇਵਲ ਕਹਾਣੀ (ਕਥਾ) ਨਹੀਂ ਹੁੰਦੀ । ਕਹਾਣੀ ਦੀ ਗੱਲ ਜਦੋਂ ਅਸੀਂ ਇਕ ਰੂਪਾਕਾਰ ਦੇ ਦ੍ਰਿਸ਼ਟੀਕੋਣ ਤੋਂ ਕਰਦੇ ਹਾਂ ਤਾਂ ਇਹ ਵੀ ਕਥਾ ਦੇ ਪਰੰਪਰਾਈ ਰੂਪ ਨੂੰ ਤਜ ਕੇ ਨਵੀਆਂ ਸੰਭਾਵਨਾਵਾਂ ਨਾਲ ਜਾ ਜੁੜਦੀ ਹੈ ।\n",
      "ਬਿਰਤਾਂਤ ਦੀ ਬਿਰਤੀ ਵਧੇਰੇ ਕਰਕੇ \n"
     ]
    }
   ],
   "source": [
    "text = open('ExtractedText.txt', encoding='utf-8').read()\n",
    "print(text[:500]) #Print the first 500 letters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a371336",
   "metadata": {},
   "source": [
    "Split corpus into sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "98900d9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sentences: 5786\n",
      "['ਕੁੱਝ ਵਿਦਵਾਨਾਂ ਲਈ ਤਾਂ ‘ਕਹਾਣੀ ਸਹਿਜ ਸਾਧਾਰਣ ਸੱਚ ਨੂੰ ਸਰਲ ਸਪਾਟ ਰੂਪ ਵਿਚ ਕਹਿੰਦੀ ਹੈ’ ਅਤੇ ‘ਏਸ ਕਲਾ ਨੂੰ... ਕਿਸੇ ਗੁੰਝਲਦਾਰ ਸਾਹਿਤ ਸ਼ਾਸਤਰ ਦੀ ਮੁਹਤਾਜ਼ੀ ਨਹੀਂ', '’ ਪਰ ਸਹਿਜ ਸਾਧਾਰਣ ਸੱਚ ਨੂੰ ਨਾ ਕਹਾਣੀ ਸਰਲ ਸਪਾਟ ਰੂਪ ਵਿਚ ਪੇਸ਼ ਕਰਦੀ ਹੈ ਅਤੇ ਨਾ ਹੀ ਇਸ ਦਾ ਸਾਹਿਤ ਸ਼ਾਸਤਰ ਏਨਾ ਸਰਲ ਰਹੇਗਾ', 'ਜਿਸ ਤਰ੍ਹਾਂ ਕਵਿਤਾ ਕੇਵਲ ਛੰਦ ਪ੍ਰਬੰਧ ਨਹੀਂ ਹੁੰਦੀ, ਇਸੇ ਤਰ੍ਹਾਂ ਕਹਾਣੀ ਕੇਵਲ ਕਹਾਣੀ (ਕਥਾ) ਨਹੀਂ ਹੁੰਦੀ']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "sentenceEnd = ['|', '।']\n",
    "pattern = f\"[{''.join(re.escape(ch) for ch in sentenceEnd)}]\"\n",
    "corpus = re.split(pattern, text)\n",
    "corpus = [s.strip() for s in corpus if s.strip()]\n",
    "print(\"Total sentences:\", len(corpus))\n",
    "print(corpus[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b4e7d8",
   "metadata": {},
   "source": [
    "Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bd69a5c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ਕੁੱਝ ਵਿਦਵਾਨਾਂ ਲਈ ਤਾਂ ਕਹਾਣੀ ਸਹਿਜ ਸਾਧਾਰਣ ਸੱਚ ਨੂੰ ਸਰਲ ਸਪਾਟ ਰੂਪ ਵਿਚ ਕਹਿੰਦੀ ਹੈ ਅਤੇ ਏਸ ਕਲਾ ਨੂੰ ਕਿਸੇ ਗੁੰਝਲਦਾਰ ਸਾਹਿਤ ਸ਼ਾਸਤਰ ਦੀ ਮੁਹਤਾਜ਼ੀ ਨਹੀਂ', ' ਪਰ ਸਹਿਜ ਸਾਧਾਰਣ ਸੱਚ ਨੂੰ ਨਾ ਕਹਾਣੀ ਸਰਲ ਸਪਾਟ ਰੂਪ ਵਿਚ ਪੇਸ਼ ਕਰਦੀ ਹੈ ਅਤੇ ਨਾ ਹੀ ਇਸ ਦਾ ਸਾਹਿਤ ਸ਼ਾਸਤਰ ਏਨਾ ਸਰਲ ਰਹੇਗਾ', 'ਜਿਸ ਤਰ੍ਹਾਂ ਕਵਿਤਾ ਕੇਵਲ ਛੰਦ ਪ੍ਰਬੰਧ ਨਹੀਂ ਹੁੰਦੀ ਇਸੇ ਤਰ੍ਹਾਂ ਕਹਾਣੀ ਕੇਵਲ ਕਹਾਣੀ ਕਥਾ ਨਹੀਂ ਹੁੰਦੀ']\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "punctuations = string.punctuation + '।' + '\\n' + '‘’'\n",
    "def cleanSentence(sentence):\n",
    "    return sentence.translate(str.maketrans('', '', punctuations))\n",
    "\n",
    "cleanedSentences = [cleanSentence(s) for s in corpus]\n",
    "print(cleanedSentences[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b319e13a",
   "metadata": {},
   "source": [
    "Import Stopwords, tokenise sentences and remove stopwords from corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "19b16bba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['ਕੁੱਝ', 'ਵਿਦਵਾਨਾਂ', 'ਕਹਾਣੀ', 'ਸਹਿਜ', 'ਸਾਧਾਰਣ', 'ਸੱਚ', 'ਸਰਲ', 'ਸਪਾਟ', 'ਰੂਪ', 'ਵਿਚ', 'ਕਹਿੰਦੀ', 'ਕਲਾ', 'ਕਿਸੇ', 'ਗੁੰਝਲਦਾਰ', 'ਸਾਹਿਤ', 'ਸ਼ਾਸਤਰ', 'ਦੀ', 'ਮੁਹਤਾਜ਼ੀ', 'ਨਹੀਂ'], ['ਸਹਿਜ', 'ਸਾਧਾਰਣ', 'ਸੱਚ', 'ਨਾ', 'ਕਹਾਣੀ', 'ਸਰਲ', 'ਸਪਾਟ', 'ਰੂਪ', 'ਵਿਚ', 'ਪੇਸ਼', 'ਕਰਦੀ', 'ਨਾ', 'ਇਸ', 'ਸਾਹਿਤ', 'ਸ਼ਾਸਤਰ', 'ਏਨਾ', 'ਸਰਲ', 'ਰਹੇਗਾ'], ['ਜਿਸ', 'ਤਰ੍ਹਾਂ', 'ਕਵਿਤਾ', 'ਕੇਵਲ', 'ਛੰਦ', 'ਪ੍ਰਬੰਧ', 'ਨਹੀਂ', 'ਹੁੰਦੀ', 'ਇਸੇ', 'ਤਰ੍ਹਾਂ', 'ਕਹਾਣੀ', 'ਕੇਵਲ', 'ਕਹਾਣੀ', 'ਕਥਾ', 'ਨਹੀਂ', 'ਹੁੰਦੀ']]\n"
     ]
    }
   ],
   "source": [
    "with open('Stopwords.txt', 'r', encoding='utf-8') as f:\n",
    "    stopWords = [word.strip('\"') for word in f.read().split(',')]\n",
    "\n",
    "from indicnlp.tokenize import indic_tokenize\n",
    "\n",
    "sentencesWithoutStopwords = []\n",
    "for sentence in cleanedSentences:\n",
    "    tokens = indic_tokenize.trivial_tokenize(sentence)\n",
    "    filteredTokens = [token for token in tokens if token not in stopWords]\n",
    "    sentencesWithoutStopwords.append(filteredTokens)\n",
    "\n",
    "print(sentencesWithoutStopwords[:3])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447fb99c",
   "metadata": {},
   "source": [
    "Import Lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "65cd8f52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  English Word  anger  anticipation  disgust  fear  joy  negative  positive  \\\n",
      "0       abacus      0             0        0     0    0         0         0   \n",
      "1      abandon      0             0        0     1    0         1         0   \n",
      "2    abandoned      1             0        0     1    0         1         0   \n",
      "3  abandonment      1             0        0     1    0         1         0   \n",
      "4         abba      0             0        0     0    0         0         1   \n",
      "\n",
      "   sadness  surprise  trust Punjabi Word  \n",
      "0        0         0      1       abacus  \n",
      "1        1         0      0     ਛੱਡ ਦੇਣਾ  \n",
      "2        1         0      0    ਛੱਡ ਦਿੱਤਾ  \n",
      "3        1         1      0         ਤਿਆਗ  \n",
      "4        0         0      0         ਅੱਬਾ  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "emotions = ['anger', 'anticipation', 'disgust', 'fear', 'joy', \n",
    "            'negative', 'positive', 'sadness', 'surprise', 'trust']\n",
    "\n",
    "lexiconDf = pd.read_csv('Punjabi-NRC-EmoLex.txt', sep='\\t', encoding='utf-8')\n",
    "\n",
    "#Filter out rows that are 0 for all emotions\n",
    "lexiconFiltered = lexiconDf[lexiconDf[emotions].sum(axis=1) > 0]\n",
    "lexiconFiltered = lexiconFiltered.reset_index(drop=True)\n",
    "\n",
    "print(lexiconFiltered.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "340097a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fear', 'negative', 'sadness']\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "emotionLexicon = defaultdict(list)\n",
    "for _, row in lexiconDf.iterrows():\n",
    "    punjabi_word = str(row['Punjabi Word']).strip()\n",
    "    for emotion in emotions:\n",
    "        if row[emotion] == 1:\n",
    "            emotionLexicon[punjabi_word].append(emotion)\n",
    "\n",
    "#Test to see if everything is loaded correctly\n",
    "print(emotionLexicon['ਛੱਡ ਦੇਣਾ']) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa5a6e7",
   "metadata": {},
   "source": [
    "Generate features and generate labels through the lexicon per sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "385463ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "def autoLabel(tokens, lexicon):\n",
    "    emoCounts = Counter()\n",
    "    for token in tokens:\n",
    "        for em in lexicon.get(token, []):\n",
    "            emoCounts[em] = emoCounts[em] + 1\n",
    "    if not emoCounts:\n",
    "        return 'neutral'  \n",
    "    return emoCounts.most_common(1)[0][0]\n",
    "\n",
    "def getEmotionVector(tokens):\n",
    "    counts = Counter()\n",
    "    for token in tokens:\n",
    "        for em in emotionLexicon.get(token, []):\n",
    "            counts[em] += 1\n",
    "    return np.array([counts.get(em, 0) for em in emotions])\n",
    "\n",
    "# Features and labels\n",
    "xFeatures = np.array([getEmotionVector(tokens) for tokens in sentencesWithoutStopwords])\n",
    "yLabels = [autoLabel(tokens, emotionLexicon) for tokens in sentencesWithoutStopwords]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5941949c",
   "metadata": {},
   "source": [
    "Encode Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "322192a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder() \n",
    "yEncoded = le.fit_transform(yLabels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068dbd11",
   "metadata": {},
   "source": [
    "Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "404164bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xFeatures shape: (5786, 10)\n",
      "yEncoded shape: (5786,)\n"
     ]
    }
   ],
   "source": [
    "print(\"xFeatures shape:\", xFeatures.shape)\n",
    "print(\"yEncoded shape:\", yEncoded.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c119f5",
   "metadata": {},
   "source": [
    "Split data into training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4c7e09ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "xTrain, xPred, yTrain, yPred = train_test_split(xFeatures, yEncoded, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac31776c",
   "metadata": {},
   "source": [
    "Build and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "03499984",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m290/290\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 785us/step - accuracy: 0.3291 - loss: 2.2955\n",
      "Epoch 2/10\n",
      "\u001b[1m290/290\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 824us/step - accuracy: 0.7252 - loss: 1.5304\n",
      "Epoch 3/10\n",
      "\u001b[1m290/290\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 848us/step - accuracy: 0.7689 - loss: 0.9741\n",
      "Epoch 4/10\n",
      "\u001b[1m290/290\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 809us/step - accuracy: 0.7967 - loss: 0.7381\n",
      "Epoch 5/10\n",
      "\u001b[1m290/290\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 805us/step - accuracy: 0.8192 - loss: 0.5978\n",
      "Epoch 6/10\n",
      "\u001b[1m290/290\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 781us/step - accuracy: 0.8255 - loss: 0.5472\n",
      "Epoch 7/10\n",
      "\u001b[1m290/290\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 771us/step - accuracy: 0.8408 - loss: 0.4798\n",
      "Epoch 8/10\n",
      "\u001b[1m290/290\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 791us/step - accuracy: 0.8402 - loss: 0.4470\n",
      "Epoch 9/10\n",
      "\u001b[1m290/290\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 847us/step - accuracy: 0.8411 - loss: 0.4280\n",
      "Epoch 10/10\n",
      "\u001b[1m290/290\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 818us/step - accuracy: 0.8448 - loss: 0.4058\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1a54411ea80>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "model = models.Sequential([\n",
    "    layers.Input(shape=(len(emotions),)),\n",
    "    layers.Dense(16, activation='relu'),\n",
    "    layers.Dense(8, activation='relu'),\n",
    "    layers.Dense(len(set(yEncoded)), activation='softmax') \n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(xTrain, yTrain, epochs=10, batch_size=16)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab459abc",
   "metadata": {},
   "source": [
    "Test prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8f119345",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "Predicted: ['positive']\n"
     ]
    }
   ],
   "source": [
    "sample = \"ਮੈਂ ਬਹੁਤ ਖੁਸ਼ ਹਾਂ\"\n",
    "tokens = indic_tokenize.trivial_tokenize(sample)\n",
    "features = getEmotionVector(tokens).reshape(1, -1)\n",
    "pred_class = model.predict(features).argmax(axis=1)\n",
    "print(\"Predicted:\", le.inverse_transform(pred_class))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
